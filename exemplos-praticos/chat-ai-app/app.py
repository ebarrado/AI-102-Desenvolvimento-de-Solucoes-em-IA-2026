import streamlit as st
import os
from dotenv import load_dotenv
from openai import AzureOpenAI

# Carrega configura√ß√µes
load_dotenv()

# Configura√ß√µes - AGORA CORRETAS para seu ambiente
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
MODEL_DEPLOYMENT = os.getenv("MODEL_DEPLOYMENT", "modelo-chat-barrado")
API_VERSION = "2024-12-01-preview"  # Vers√£o da sua imagem

# Inicializa cliente do OpenAI com CHAVE DE  API
@st.cache_resource
def get_openai_client():
    """
    Cria e retorna um cliente Azure OpenAI autenticado com chave de API
    """
    try:
        if not AZURE_OPENAI_KEY:
            st.error("‚ùå Chave de API n√£o encontrada. Verifique seu arquivo .env")
            return None
            
        client = AzureOpenAI(
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            api_key=AZURE_OPENAI_KEY,
            api_version=API_VERSION
        )
        return client
    except Exception as e:
        st.error(f"Erro na autentica√ß√£o: {str(e)}")
        return None

# Interface Streamlit
st.set_page_config(
    page_title="Chat IA Generativa - Azure AI Foundry",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Chat IA Generativa - Azure AI Foundry")
st.caption(f"Powered by **{MODEL_DEPLOYMENT}** via Azure OpenAI")

# Inicializa√ß√£o do hist√≥rico
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": 
            "Voc√™ √© uma professora t√©cnica de tecnologia. "
            "Explique de forma clara, objetiva e did√°tica. "
            "Use linguagem simples e exemplos pr√°ticos. "
            "Responda em portugu√™s do Brasil."}
    ]

# Sidebar para configura√ß√µes e informa√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Controles de temperatura
    temperature = st.slider(
        "Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.1,
        help="Controla a criatividade das respostas"
    )
    
    max_tokens = st.slider(
        "M√°ximo de tokens",
        min_value=100,
        max_value=4000,
        value=1000,
        step=100,
        help="Tamanho m√°ximo da resposta"
    )
    top_p = st.slider(
        "Top P",
        min_value=0.1,
        max_value=1.0,
        value=0.95,
        step=0.05
    )   
    
    if st.button("üóëÔ∏è Limpar Conversa", use_container_width=True):
        st.session_state.messages = [
            {"role": "system", "content": "Voc√™ √© um assistente √∫til e responde em portugu√™s do Brasil."}
        ]
        st.rerun()
    
    st.divider()
    
    # Informa√ß√µes do sistema
    st.header("‚ÑπÔ∏è Informa√ß√µes da Implanta√ß√£o")
    
    # Verifica status da conex√£o
    client = get_openai_client()
    status = "‚úÖ Conectado" if client else "‚ùå Desconectado"
    
    st.info(f"""
    **Modelo:** {MODEL_DEPLOYMENT}
    **Endpoint:** {AZURE_OPENAI_ENDPOINT.split('//')[1].split('.')[0] if AZURE_OPENAI_ENDPOINT else 'N√£o configurado'}
    **Status:** {status}
    **Vers√£o API:** {API_VERSION}
    **Limite:** 50K tokens/min
    """)
    
    st.divider()
    
    # Dicas
    with st.expander("üìò Dicas de uso"):
        st.markdown("""
        - **Temperatura baixa** (0.0-0.3): Respostas mais precisas e consistentes
        - **Temperatura m√©dia** (0.4-0.7): Bom equil√≠brio entre criatividade e precis√£o
        - **Temperatura alta** (0.8-1.0): Respostas mais criativas e variadas
        """)

# √Årea principal do chat
chat_container = st.container()

with chat_container:
    # Exibe hist√≥rico (pulando mensagens do sistema)
    for message in st.session_state.messages:
        if message["role"] != "system":
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

# Input do usu√°rio
if prompt := st.chat_input("üí¨ Digite sua mensagem..."):
    # Adiciona mensagem do usu√°rio
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Gera resposta
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("ü§î Pensando..."):
            try:
                client = get_openai_client()
                
                if client is None:
                    st.error("‚ùå Cliente n√£o inicializado. Verifique suas credenciais.")
                    st.stop()
                
                # Prepara mensagens
                messages_for_api = st.session_state.messages.copy()
                
                # Faz a chamada √† API com STREAMING
                response = client.chat.completions.create(
                    model=MODEL_DEPLOYMENT,
                    messages=messages_for_api,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    stream=True
                )
                
                # Processa streaming
                full_response = ""
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        message_placeholder.markdown(full_response + "‚ñå")
                
                message_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                error_message = str(e)
                st.error(f"‚ùå Erro na gera√ß√£o da resposta")
                
                # Tratamento espec√≠fico de erros
                if "401" in error_message:
                    st.warning("üîë **Erro de autentica√ß√£o**: Verifique se sua chave de API est√° correta no arquivo .env")
                elif "404" in error_message:
                    st.warning(f"üîç **Modelo n√£o encontrado**: Verifique se o nome '{MODEL_DEPLOYMENT}' est√° correto")
                elif "429" in error_message:
                    st.warning("‚è≥ **Limite de taxa excedido**: Aguarde um momento (limite: 50K tokens/min)")
                elif "Connection" in error_message:
                    st.warning(f"üåê **Erro de conex√£o**: Verifique se o endpoint '{AZURE_OPENAI_ENDPOINT}' est√° acess√≠vel")
                else:
                    st.info(f"Detalhes: {error_message[:200]}...")

# Rodap√©
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"üîí Autentica√ß√£o: **Chave de API**")
with col2:
    st.caption(f"ü§ñ Modelo: **{MODEL_DEPLOYMENT}**")
with col3:
    st.caption(f"üìä Limite: **50K tokens/min**")