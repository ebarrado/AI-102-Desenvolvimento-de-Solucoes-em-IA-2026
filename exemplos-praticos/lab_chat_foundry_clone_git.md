# Crie um aplicativo cliente para conversar com o modelo

1. No portal do Foundry, visualzie a pÃ¡gina **VisÃ£o Geral** do seu projeto
2. Em **Endpoints e chaves**, certifique que a biblioteca Foundry esteja selecionada e visualize o endpoint do projeto Foundry
3. Abra o VS Code
4. Clique em **Terminal - New Terminal**
5. Acesse um diretÃ³rio para clonar a pasta do GitHub
6. Insira os seguinte comando para clonar o repositÃ³rio do GitHub

```bash
rm -rf mslearn-ai-foundry  # Remove se existir
git clone https://github.com/microsoftlearning/mslearn-ai-studio mslearn-ai-foundry
```
6. Aguarde
7. No menu **File-Open Folder**, selecione a pasta clonada

![alt text](/imagens/repositorio_git.png)

8. Acesse a pasta que contÃ©m os arquivos de cÃ³digo do aplicativo de bate-papo e visualize-os:`mslearn-ai-foundry/labfiles/chat-app/python`

> A pasta pode ser acessada via terminal usando os comandos:

```bash
cd labfiles/chat-app/python
ls -la #lista conteÃºdo da pasta
```

## Instalar as bibliotecas

```bash
#acesse a pasta do projeto
pip install -r requirements.txt
pip install -r requirements.txt azure-identity azure-ai-projects openai

# se precisar atualizar 
python.exe -m pip install --upgrade pip
# execute novamente - pip install -r requirements.txt azure-identity azure-ai-projects openai
```

9. No Terminal, digite o seguinte comando no diretorio raiz, para criar e ativar `venv`

```bash
python -m venv labenv
```
> O VS Code detecta que um novo ambiente virtual foi criado e pergunta se vocÃª quer usÃ¡-lo como interpretador padrÃ£o do projeto.


![alt text](/imagens/notificacao.png)

* Clique em `Yes` - Isso farÃ¡ com que:
    * O VS Code use o Python do labenv
    * As bibliotecas instaladas fiquem isoladas nesse projeto
    * O terminal jÃ¡ abra com o ambiente correto

## Ativar

```bash
labenv\Scripts\activate
# No Linux/Mac: source labenv/bin/activate
```

## ConfiguraÃ§Ã£o do arquivo `.env`

1. Abra `.env` no VS Code (Ctrl+P, digite .env). Substitua:

* `your_project_endpoint` pelo endpoint do Foundry (VisÃ£o geral > Endpoints e chaves)
* `your_model_deployment` por gpt-4o (ou nome da implantaÃ§Ã£o).


```txt
PROJECT_ENDPOINT=your_project_endpoint
MODEL_DEPLOYMENT=your_model_deployment 
```

## Escrevendo o cÃ³digo para conectar ao projeto e conversar com o modelo

1. Abra o arquivo `chat-app.py`
2. Adicione o seguinte cÃ³digo para referÃªnciar aos namespaces nas bibliotecas que vocÃª instalou

```python

# Add references
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from openai import AzureOpenAI

```

3. Na funÃ§Ã£o principal, comentÃ¡rio `Obter ConfiguraÃ§Ãµes` (Inicialize the project client)

```python
# Initialize the project client
project_client = AIProjectClient(            
         credential=DefaultAzureCredential(
             exclude_environment_credential=True,
             exclude_managed_identity_credential=True
         ),
         endpoint=project_endpoint,
     )
```
4. Localize o comentÃ¡rio "Obter um cliente de bate-papo", adicione:

```python

â€‹# Get a chat client
openai_client = project_client.get_openai_client(api_version="2024-10-21")
```
5. Adicione ao comentÃ¡rio "Initialize prompt with system message":

```python

# Initialize prompt with system message
prompt = [
        {"role": "system", "content": 
        "VocÃª Ã© um assistente educacional claro, didÃ¡tico e objetivo. "
        "Responda em portuguÃªs do Brasil. "
        "Explique conceitos de forma simples e use exemplos quando possÃ­vel."
}
     ]
```


. Na seÃ§Ã£o do loop, encontre o comentÃ¡rio " Obter uma conclusÃ£o do chat" e adicione

```python
# Get a chat completion
prompt.append({"role": "user", "content": input_text})
response = openai_client.chat.completions.create(
         model=model_deployment,
         messages=prompt)
completion = response.choices[0].message.content
print(completion)
prompt.append({"role": "assistant", "content": completion})
```

7. Salve

## Iniciar sessÃ£o no Azure e executar a AplicaÃ§Ã£o

1. No terminal execute o comando
```bash
az login
```
* Ative a credencial Azure

2. Execute o aplicativo com o comando:

```bash
python chat-app.py
```

# Controlar temperatura (mais controle de respostas)

Para melhorar a estabilidade adicionando parÃ¢metros: `temperature`, `max_tokens=800`.

* Substitua as linhas:

```python
response = openai_client.chat.completions.create(
    model=model_deployment,
    messages=prompt)
```

* Por:

```python
response = openai_client.chat.completions.create(
    model=model_deployment,
    messages=prompt,
    temperature=0.3,   # mais controlado
    max_tokens=800
)
```
* temperature=0.3 â†’ respostas mais tÃ©cnicas e estÃ¡veis

* max_tokens=800 â†’ evita respostas gigantes

* top_p=0.95 â†’ mantÃ©m diversidade controlada

## Recomendado

| Objetivo           | temperature |
| ------------------ | ----------- |
| Respostas tÃ©cnicas | 0.2 â€“ 0.4   |
| Chat equilibrado   | 0.5 â€“ 0.7   |
| Criatividade alta  | 0.8 â€“ 1.0   |


# Exemplo

```python
prompt = [
            {"role": "system", "content": 
            "VocÃª Ã© uma professora tÃ©cnica de tecnologia. "
            "Explique de forma clara, objetiva e didÃ¡tica. "
            "Use linguagem simples e exemplos prÃ¡ticos. "
            "Responda em portuguÃªs do Brasil."
}
        ]
```

* Execute: python chat-app.py

* Pergunta : `Crie uma analogia para explicar o que Ã© Machine Learning.`

* Resposta:
```text
Digite uma mensagem (ou digite 'sair' para encerrar): Crie uma analogia para explicar o que Ã© Machine Learning.
Claro! Aqui vai uma analogia prÃ¡tica para entender o que Ã© **Machine Learning**: Imagine que vocÃª estÃ¡ ensinando um amigo a identificar frutas sÃ³ olhando para elas. Esse amigo Ã© como uma "mÃ¡quina" que precisa aprender observando exemplos. 1. **Treinamento (Aprendendo com exemplos):** Primeiro, vocÃª mostra vÃ¡rias frutas para o seu amigo. VocÃª aponta para uma maÃ§Ã£ e diz: "Isso Ã© uma maÃ§Ã£". Depois, vocÃª mostra uma banana e diz: "Isso Ã© uma banana". O mesmo acontece com laranjas, uvas e outras frutas. Ou seja, vocÃª estÃ¡ fornecendo **dados de entrada (imagens das frutas)** e o **rÃ³tulo correto (o nome de cada fruta)**. 2. **Reconhecimento (Fase de aprendizado):** Com o tempo, seu amigo comeÃ§a a perceber padrÃµes. Ele aprende, por exemplo, que maÃ§Ã£s geralmente tÃªm uma forma redonda e sÃ£o vermelhas ou verdes, enquanto bananas sÃ£o alongadas e amarelas. Ele **aprendeu com os exemplos**. 3. **Testando o aprendizado:** Agora vocÃª mostra uma fruta nova para o seu amigo e pergunta: "O que Ã© isso?". Ã‰ aqui que vocÃª descobre se ele aprendeu mesmo! Ele olha e diz: "Parece uma maÃ§Ã£ porque Ã© redonda e vermelha". Se ele acertar, significa que ele **generalizou bem o aprendizado**. Se errar, talvez precise de mais exemplos para corrigir. --- Agora vamos conectar isso com **Machine Learning**: - O **amigo** representa um **modelo de machine learning**. - As **frutas mostradas** sÃ£o os **dados de treinamento**. - O processo de identificar padrÃµes Ã© como o **algoritmo aprendendo** a partir dos dados. - Testar o que ele aprendeu equivale a usar o modelo para fazer **previsÃµes** em novos dados. ### Exemplo real: Um aplicativo de fotos, como o Google Fotos, usa Machine Learning desse jeito para identificar fotos de "cachorros" ou "pessoas". Ele "aprendeu" o que Ã© um cachorro olhando milhÃµes de imagens de treinamento (dados) e agora consegue identificar cachorros nas suas fotos, mesmo que nunca tenha visto aquele cÃ£o especÃ­fico antes. Espero que essa analogia tenha ajudado! ðŸ˜Š
```

### Resposta usando temperature e max_tokens e top_p

```text
Digite uma mensagem (ou digite 'sair' para encerrar): Crie uma analogia para explicar o que Ã© Machine Learning.
Claro! Vamos imaginar que vocÃª estÃ¡ ensinando um cachorro a buscar uma bolinha. Essa situaÃ§Ã£o pode ser uma Ã³tima analogia para entender o que Ã© **Machine Learning** (Aprendizado de MÃ¡quina). ### O cachorro Ã© como uma mÃ¡quina ou um computador. Ele nÃ£o sabe, por conta prÃ³pria, como buscar a bolinha. Mas, com o tempo e prÃ¡tica, ele pode aprender. ### O treinamento Ã© o processo de aprendizado. VocÃª comeÃ§a jogando a bolinha e, toda vez que o cachorro traz a bolinha de volta, vocÃª dÃ¡ um petisco (recompensa). Se ele nÃ£o traz a bolinha, vocÃª nÃ£o dÃ¡ nada. Com o tempo, ele entende que, para ganhar o petisco, precisa buscar a bolinha. ### Os dados sÃ£o as tentativas. Cada vez que vocÃª joga a bolinha e o cachorro tenta buscar, isso Ã© como um "dado" que ele usa para aprender. Quanto mais vezes ele tenta, mais ele entende o que precisa fazer. ### O modelo treinado Ã© o cachorro que aprendeu. Depois de vÃ¡rias tentativas, o cachorro jÃ¡ sabe o que fazer: ele busca a bolinha automaticamente, sem precisar de mais petiscos. Ele aprendeu o "padrÃ£o" do que vocÃª quer. --- Agora, trazendo para o mundo da tecnologia: - O **cachorro** Ã© o computador ou sistema. - O **treinamento** Ã© o processo de ensinar o sistema usando dados. - Os **dados** sÃ£o as informaÃ§Ãµes que vocÃª fornece para o sistema aprender (como fotos, nÃºmeros, textos, etc.). - O **modelo treinado** Ã© o sistema que aprendeu a realizar uma tarefa, como reconhecer rostos em fotos ou prever o clima. Assim como o cachorro precisa de prÃ¡tica para aprender, o computador precisa de muitos dados para entender padrÃµes e realizar tarefas de forma inteligente. ðŸ˜Š
```

* Com `temperature = 0.3`

Temperatura baixa =
âœ”ï¸ Mais previsÃ­vel
âœ”ï¸ Mais conservador
âœ”ï¸ Menos criatividade

> A temperatura nÃ£o escolhe a resposta.
Ela controla o nÃ­vel de variaÃ§Ã£o probabilÃ­stica.

## ðŸ”´ Ultra controlado

```python
temperature=0.1
max_tokens=200
```

* Resposta curta
* Pouca criatividade
* Mais tÃ©cnica

## ðŸŸ£ Criativo

```python
temperature=0.9
max_tokens=400
```
* MetÃ¡foras diferentes
* Linguagem mais criativa
* Resposta menos previsÃ­vel

# Top_p

`top_p`controla o nÃ­vel de diversidade das palavras escolhidas pelo modelo.
Imagine que o modelo estÃ¡ escolhendo a prÃ³xima palavra. Ele calcula vÃ¡rias possibilidades com probabilidade:

| Palavra possÃ­vel | Probabilidade |
| ---------------- | ------------- |
| cachorro         | 40%           |
| animal           | 30%           |
| pet              | 15%           |
| lobo             | 10%           |
| dragÃ£o           | 5%            |

* Se o `top_p = 0.5` 

Ele sÃ³ considera as palavras que somam atÃ© 50% de probabilidade

* No caso:
    * Cachorro (40%)
    * Animal (30%)

JÃ¡ passou de 50%, entÃ£o ele escolhe apenas entre essas duas.

### Resultado: mais previsÃ­vel.

*  Se top_p = 0.95

    * Ele considera quase todas as opÃ§Ãµes.

### Resultado: mais variedade.

| Valor | Comportamento            |
| ----- | ------------------------ |
| 0.1   | Muito conservador        |
| 0.5   | Moderadamente controlado |
| 0.9   | Criativo                 |
| 1.0   | Totalmente livre         |

## DiferenÃ§a entre `temperature`e `top_p`

| ParÃ¢metro   | O que controla               |
| ----------- | ---------------------------- |
| temperature | O quanto ele "arrisca"       |
| top_p       | Quantas opÃ§Ãµes ele considera |

> Temperature controla a ousadia.

> top_p controla o tamanho do universo de escolhas.

> Arquitetura de IA Ã© decidir o nÃ­vel de risco criativo adequado ao negÃ³cio.

# Fluxo real

1. O CÃ³digo envia requisiÃ§Ã£o HTTPS
2. Azure autentica via token
3. O Foundry localiza o modelo implantado
4. O modelo processa o prompt
5. Aplica temperatura e top_p
6. Retorna os tokens gerados
7. SDK transforma em objeto Python
8. VocÃª extrai: `response.choices[0].message.content`

> Sempre que for executar o projeto acesse a pasta e ative o ambiente virtual `labenv\Scripts\activate`